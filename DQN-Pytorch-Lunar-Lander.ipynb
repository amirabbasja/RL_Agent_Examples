{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import math\n",
    "import random\n",
    "import imageio\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use gymnasium's LuanrLander-v2 environment. According to the documentation, we have four actions in the action space, 0 (Do nnothing), 1 (Fire left orientation engine), 2 (Fire main engine)  , 3 (Fire right orientation engine)\n",
    "\n",
    "Also the observation space is a ndarray with shape (8,), It is as follows:\n",
    "- 1, 2: It's  (x,y) coordinates. The landing pad is always at coordinates (0,0).\n",
    "- 3, 4: It's linear velocities  (xDot,yDot).\n",
    "- 5: It's angle  Theta.\n",
    "- 6: It's angular velocity thetaDot\n",
    "- 7, 8: Two booleans l and r that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "Rewards: \n",
    "- is increased/decreased the closer/further the lander is to the landing pad.\n",
    "- is increased/decreased the slower/faster the lander is moving.\n",
    "- is decreased the more the lander is tilted (angle not horizontal).\n",
    "- is increased by 10 points for each leg that is in contact with the ground.\n",
    "- is decreased by 0.03 points each frame a side engine is firing.\n",
    "- is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively. An episode is considered a solution if it scores at least 200 points.\n",
    "\n",
    "Episode end: The episode finishes if:\n",
    "- The lander crashes (the lander body gets in contact with the moon);\n",
    "- The lander gets outside of the viewport (x coordinate is greater than 1);\n",
    "- The lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the environment\n",
    "env = gym.make(\"LunarLander-v2\") # Use render_mode = \"human\" to render each episode\n",
    "state, info = env.reset() # Get a sample state of the environment\n",
    "stateSize = env.observation_space.shape # Number of variables to define current step\n",
    "nActions = env.action_space.n # Number of actions\n",
    "nObs = len(state) # Number of features\n",
    "\n",
    "\n",
    "# Set pytorch parameters: The device (CPU or GPU) and data types\n",
    "__device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "__dtype = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Implement's the replay memory algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dtype, device) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the class with a double ended queue which will contain named tuples.\n",
    "        \n",
    "        Args:\n",
    "            size (int): The maximum size of the memory buffer.\n",
    "            dtype (torch.dtype): The data type of the elements in the memory buffer.\n",
    "            device (torch.device): The device to store the data (CPU or GPU)\n",
    "        \"\"\"\n",
    "        self.exp = deque([], maxlen=size)\n",
    "        self.len = len(self.exp)\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "    \n",
    "    def addNew(self, exp:namedtuple) -> None:\n",
    "        \"\"\"\n",
    "        Adding a new iteration to the memory. Note that the most recent values of\n",
    "        the training will be located at the position 0 and if the list reaches maxlen\n",
    "        the oldest data will be dropped.\n",
    "\n",
    "        Args:\n",
    "            exp: namedtuple: The experience should be a named tuple with keys named \n",
    "                like this: [\"state\", \"action\", \"reward\", \"nextState\", \"done\"]\n",
    "        \"\"\"\n",
    "        self.exp.appendleft(exp)\n",
    "        self.len = len(self.exp)\n",
    "    \n",
    "    def sample(self, miniBatchSize:int, framework = \"pytorch\") -> tuple:\n",
    "        \"\"\"\n",
    "        Get a random number of experiences from the entire experience memory.\n",
    "        The memory buffer is a double ended queue (AKA deque) of named tuples. To make \n",
    "        this list usable for tensor flow neural networks, this each named tuple inside \n",
    "        the deque has to be unpacked. we use a iterative method to unpack. It may be \n",
    "        inefficient and maybe using pandas can improve this process. one caveat of using\n",
    "        pandas tables instead of deque is expensiveness of appending/deleting rows \n",
    "        (experiences) from the table.\n",
    "\n",
    "        Args:\n",
    "            miniBatchSize: int: The size of returned the sample\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing state, action, reward, nextState and done\n",
    "        \"\"\"\n",
    "        if framework == \"pytorch\":\n",
    "            miniBatch = random.sample(self.exp, miniBatchSize)\n",
    "            state = torch.from_numpy(np.array([e.state for e in miniBatch if e != None])).to(self.device, dtype = self.dtype)\n",
    "            action = torch.from_numpy(np.array([e.action for e in miniBatch if e != None])).to(self.device, dtype = torch.int)\n",
    "            reward = torch.from_numpy(np.array([e.reward for e in miniBatch if e != None])).to(self.device, dtype = self.dtype)\n",
    "            nextState = torch.from_numpy(np.array([e.nextState for e in miniBatch if e != None])).to(self.device, dtype = self.dtype)\n",
    "            done = torch.from_numpy(np.array([e.done for e in miniBatch if e != None]).astype(np.uint8)).to(self.device, dtype = torch.int)\n",
    "        elif framework == \"tensorflow\":\n",
    "            miniBatch = random.sample(self.exp, miniBatchSize)\n",
    "            state = tf.convert_to_tensor(np.array([e.state for e in miniBatch if e != None]), dtype=tf.float32)\n",
    "            action = tf.convert_to_tensor(np.array([e.action for e in miniBatch if e != None]), dtype=tf.float32)\n",
    "            reward = tf.convert_to_tensor(np.array([e.reward for e in miniBatch if e != None]), dtype=tf.float32)\n",
    "            nextState = tf.convert_to_tensor(np.array([e.nextState for e in miniBatch if e != None]), dtype=tf.float32)\n",
    "            done = tf.convert_to_tensor(np.array([e.done for e in miniBatch if e != None]).astype(np.uint8), dtype=tf.float32)\n",
    "        return tuple((state, action, reward, nextState, done))\n",
    "\n",
    "def decayEbsilon(currE: float, rate:float, minE:float) -> float:\n",
    "    \"\"\"\n",
    "    Decreases ebsilon each time called. It multiplies current ebsilon to decrease rate.\n",
    "    The decreasing is continued until reaching minE.\n",
    "    \"\"\"\n",
    "    return(max(currE*rate, minE))\n",
    "\n",
    "def computeLoss(experiences:tuple, gamma:float, qNetwork, target_qNetwork):\n",
    "    \"\"\"\n",
    "    Computes the loss between y targets and Q values. For target network, the Q values are \n",
    "    calculated using Bellman equation. If the reward of current step is R_i, then y = R_i\n",
    "    if the episode is terminated, if not, y = R_i + gamma * Q_hat(i+1) where gamma is the\n",
    "    discount factor and Q_hat is the predicted return of the step i+1 with the \n",
    "    target_qNetwork.\n",
    "    \n",
    "    For the primary Q network, Q values are acquired from the step taken in the episode \n",
    "    experiences (Not necessarily MAX(Q value)).\n",
    "    \n",
    "    Args:\n",
    "        experiences (Tuple): A tuple containing experiences as pytorch tensors.\n",
    "        gamma (float): The discount factor.\n",
    "        qNetwork (pytorch NN): The neural network for predicting the Q.\n",
    "        target_qNetwork (pytorch NN): The neural network for predicting the target-Q.\n",
    "    \n",
    "    Returns:\n",
    "        loss: float: The Mean squared errors (AKA. MSE) of the Qs.\n",
    "    \"\"\"\n",
    "    # Unpack the experience mini-batch\n",
    "    state, action, reward, nextState, done = experiences\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    target_qNetwork.eval()\n",
    "    qNetwork.eval()\n",
    "    \n",
    "    # To implement the calculation scheme explained in comments, we multiply Qhat by (1-done).\n",
    "    # If the episode has terminated done == True so (1-done) = 0.\n",
    "    Qhat = torch.amax(target_qNetwork(nextState), dim = 1)\n",
    "    yTarget = reward + gamma *  Qhat * ((1 - done)) # Using the bellman equation\n",
    "    \n",
    "    # IMPORTANT: When getting qValues, we have to account for the ebsilon-greedy algorithm as well.\n",
    "    # This is why we dont use max(qValues in each state) but instead we use the qValues of the taken\n",
    "    # action in that step.\n",
    "    qValues = qNetwork(state)\n",
    "    \n",
    "    qValues = qValues[torch.arange(state.shape[0], dtype = torch.long), action]\n",
    "        \n",
    "    # Calculate the loss\n",
    "    loss = nn.functional.mse_loss(qValues, yTarget)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def fitQNetworks(experience, gamma, qNetwork, target_qNetwork):\n",
    "    \"\"\"\n",
    "    Updates the weights of the neural networks with a custom training loop. The target network is\n",
    "    updated by a soft update mechanism.\n",
    "\n",
    "    Args: \n",
    "        experience (tuple): The data for training networks. This data has to be passed with \n",
    "            replayMemory.sample() function which returns a tuple of tensorflow tensors in \n",
    "            the following order: state, action, reward, nextState, done)\n",
    "        gamma (float): The learning rate.\n",
    "        qNetwork, target_qNetwork (list): A list of pytorch model and its respective \n",
    "            optimizer. The first member should be the model, second one its optimizer\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    __qNetworkModel = qNetwork[0]\n",
    "    __qNetworkOptim = qNetwork[1]\n",
    "    __targetQNetworkModel = target_qNetwork[0]\n",
    "    \n",
    "    # Update the Q network's weights\n",
    "    loss = computeLoss(experience, gamma, __qNetworkModel, __targetQNetworkModel)\n",
    "    \n",
    "    __qNetworkModel.train()\n",
    "    __targetQNetworkModel.train()\n",
    "    \n",
    "    __qNetworkOptim.zero_grad()\n",
    "    loss.backward()\n",
    "    __qNetworkOptim.step()\n",
    "    \n",
    "    # Update the target Q network's weights using soft updating method    \n",
    "    for targetParams, primaryParams in zip(__targetQNetworkModel.parameters(), __qNetworkModel.parameters()):\n",
    "        targetParams.data.copy_(targetParams.data * (1 - .001) + primaryParams.data * .001)\n",
    "\n",
    "def getAction(qVal: list, e:float) -> int:\n",
    "    \"\"\"\n",
    "    Gets the action via an epsilon-greedy algorithm. This entire action state is [0, 1, 2, 3].\n",
    "    With a probability of epsilon, a random choice will be picked, else the action with\n",
    "    the greatest Q value will be picked\n",
    "\n",
    "    Args:\n",
    "        qVal: list: The q value of actions\n",
    "        e: float: The epsilon which represents the probability of a random action\n",
    "\n",
    "    Returns:\n",
    "        action_: int: 0 for doing nothing, and 1 for left thruster, 2 form main thruster\n",
    "            and 3 for right thruster.\n",
    "    \"\"\"\n",
    "    rnd = random.random()\n",
    "\n",
    "    # The actions possible for LunarLander i.e. [DoNothing, leftThruster, MainThruster, RightThruster]\n",
    "    actions = [0, 1, 2, 3] \n",
    "\n",
    "    if rnd < e:\n",
    "        # Take a random step\n",
    "        action_ = random.randint(0,3)\n",
    "    else:\n",
    "        action_ = actions[torch.argmax(qVal)]\n",
    "    \n",
    "    return action_\n",
    "\n",
    "def updateNetworks(timeStep: int, replayMem: ReplayMemory, miniBatchSize: int, C: int) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the neural network (qNetwork and target_qNetwork) weights are to be updated.\n",
    "    The update happens C time steps apart. for performance reasons.\n",
    "\n",
    "    Args:\n",
    "        timeStep: int: The time step of the current episode\n",
    "        replayMem: deque: A double edged queue containing the experiences as named tuples.\n",
    "            the named tuples should be as follows: [\"state\", \"action\", \"reward\", \"nextState\", \"done\"]\n",
    "\n",
    "    Returns:\n",
    "        A boolean, True for update and False to not update.\n",
    "    \"\"\"\n",
    "\n",
    "    return True if ((timeStep+1) % C == 0 and miniBatchSize < replayMem.len) else False\n",
    "\n",
    "def getEbsilon(e:float, eDecay:float, minE: float) -> float:\n",
    "    \"\"\"\n",
    "    Decay epsilon for epsilon-Greedy algorithm. epsilon starts with 1 at the beginning of the \n",
    "    learning process which indicates that the agent completely acts on a random basis (AKA \n",
    "    Exploration) but as the learning is continued, the rate at which agent acts randomly decreased\n",
    "    via multiplying the epsilon by a decay rate which ensures agent acting based on it's learnings\n",
    "    (AKA Exploitation).\n",
    "\n",
    "    Args:\n",
    "        e: float: The current rate of epsilon\n",
    "        eDecay: float: The decay rate of epsilon\n",
    "        minE: float: the minimum amount of epsilon. To ensure the exploration possibility of the \n",
    "            agent, epsilon should't be less than a certain amount.\n",
    "\n",
    "    Returns: epsilon's value\n",
    "    \"\"\"\n",
    "\n",
    "    return max(minE, eDecay * e)\n",
    "\n",
    "def renderEpisode(initialState: int, actions:str, envName:str, delay:float = .02) -> None:\n",
    "    \"\"\"\n",
    "    Renders the previously done episode so the user can see what happened. We use Gym to \n",
    "    render the environment. All the render is done in the \"human\" mode.\n",
    "\n",
    "    Args:\n",
    "        initialState: int: The initial seed that determine's the initial state of the episode \n",
    "            (The state before we took teh first action)\n",
    "        actions: string: A string of actions delimited by comma (i.e. 1,2,3,1,3, etc.)\n",
    "        env: string: The name of the environment to render the actions, It has to be a gymnasium\n",
    "            compatible environment.\n",
    "        delay: int: The delay (In seconds) to put between showing each step to make it more \n",
    "            comprehensive.\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    tempEnv = gym.make(envName, render_mode = \"human\") # Use render_mode = \"human\" to render each episode\n",
    "    state, info = tempEnv.reset(seed=initialState) # Get a sample state of the environment\n",
    "\n",
    "    # Process the string of actions taken\n",
    "    actions = actions.split(\",\") # Split the data\n",
    "    actions = actions[:-1] # Remove the lat Null member of the list\n",
    "    actions = list(map(int, actions)) # Convert the strings to ints\n",
    "\n",
    "    # Take steps\n",
    "    for action in actions:\n",
    "        _, _, terminated, truncated, _ = tempEnv.step(action)\n",
    "    \n",
    "        # Exit loop if the simulation has ended\n",
    "        if terminated or truncated:\n",
    "            _, _ = tempEnv.reset()\n",
    "            break\n",
    "        \n",
    "        # Delay showing the next step\n",
    "        time.sleep(delay)\n",
    "\n",
    "    tempEnv.close()\n",
    "\n",
    "def analyzeLearning(episodePointHistory:list, episodeTimeHistory:list) -> None:\n",
    "    \"\"\"\n",
    "    Plots the learning performance of the agent\n",
    "\n",
    "    Args:\n",
    "        episodePointHistory: list: The commulative rewards of each episode in consrcutive time steps.\n",
    "        episodeTimeHistory: list: The time it took to run the episode\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 5))\n",
    "    ax1.plot(episodePointHistory)\n",
    "    ax1.set_title(\"Episode points\")\n",
    "\n",
    "    ax2.plot(episodeTimeHistory)\n",
    "    ax2.set_title(\"Episode elapsed time\")\n",
    "\n",
    "def testAgent(envName:str, network, saveVideoName:str = \"\") -> int:\n",
    "    \"\"\"\n",
    "    Runs an agent through a predefined gymnasium environment. The actions of the agent are chosen via\n",
    "    a greedy policy by a trained neural network. To see the agent in action, the environment's render\n",
    "    mode has to be \"human\" or  \"rgb-array\"\n",
    "\n",
    "    Args:   \n",
    "        envName: string: The name of the environment.\n",
    "        network: pytorch NN: The trained neural network that accepts state as an input and outputs\n",
    "            the desired action.\n",
    "        environment: gymnasium env: The environment for testing.\n",
    "        saveVideoName:string: The name of the file to be saved. If equals \"\", No video file will be \n",
    "            saved; Also remember that the file name should include the file extension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def interactionLoop(env_, seed_, V_):    \n",
    "        \"\"\"\n",
    "        The loop that lets agent interact with the environment.\n",
    "        if V_ == True, save the video (requires render_mode == rgb_array)\n",
    "        \"\"\"\n",
    "        state, _ = env_.reset(seed = seed_)\n",
    "        points = 0\n",
    "        if V_:\n",
    "            videoWriter = imageio.get_writer(saveVideoName)\n",
    "\n",
    "        maxStepN = 1000\n",
    "        for t in range(maxStepN):\n",
    "            # Take greedy steps\n",
    "            # action = np.argmax(network(np.expand_dims(state, axis = 0)))\n",
    "            \n",
    "            action = torch.argmax(network(torch.tensor(state, device = __device, dtype = __dtype)))\n",
    "            \n",
    "            state, reward, terminated, truncated, _ = env_.step(action.item())\n",
    "\n",
    "            if V_:\n",
    "                videoWriter.append_data(env_.render())\n",
    "\n",
    "            points += reward\n",
    "\n",
    "            # Exit loop if the simulation has ended\n",
    "            if terminated or truncated:\n",
    "                _, _ = env_.reset()\n",
    "\n",
    "                if V_:\n",
    "                    videoWriter.close()\n",
    "                    \n",
    "                return points\n",
    "    \n",
    "    # Get the random seed to get the initial state of the agent.\n",
    "    seed = random.randint(0, 1_000_000_000)\n",
    "    \n",
    "    # Because gymnasium doesn't let the environment to have two render modes, \n",
    "    # we run the simulation twice, The first renders the environment with \"human\"\n",
    "    # mode and the second run, runs the environment with \"egb_array\" mode that \n",
    "    # lets us save the interaction process to a video file. Both loops are run \n",
    "    # with the same seeds and neural networks so they should have identical outputs.\n",
    "    environment = gym.make(envName, render_mode = \"human\")\n",
    "    point = interactionLoop(environment, seed, False)\n",
    "    \n",
    "    environment = gym.make(envName, render_mode = \"rgb_array\")\n",
    "    point = interactionLoop(environment, seed, True)\n",
    "\n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qNetwork(nn.Module):\n",
    "    def __init__(self, inputSize, L1Size, L2Size, outputSize):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(inputSize, L1Size)\n",
    "        self.L1Relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(L1Size, L2Size)\n",
    "        self.L2Relu = nn.ReLU()\n",
    "        self.output = nn.Linear(L2Size, outputSize)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.L1Relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.L2Relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "qNetwork_model = qNetwork(stateSize[0], 64, 64, nActions).to(__device, dtype = __dtype)\n",
    "targetQNetwork_model = qNetwork(stateSize[0], 64, 64, nActions).to(__device, dtype = __dtype)\n",
    "\n",
    "# Two models should have identical weights initially\n",
    "targetQNetwork_model.load_state_dict(qNetwork_model.state_dict())\n",
    "\n",
    "# TODO: Add gradient clipping to the optimizer for avoiding exploding gradients\n",
    "# Suitable optimizer for gradient descent\n",
    "optimizer_main = torch.optim.Adam(qNetwork_model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "optimizer_target = torch.optim.Adam(targetQNetwork_model.parameters(), lr=0.001, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElapsedTime: 450s | Episode: 391 | The average of the 100 episodes is: 52.07 | loss: None   \n",
      "The learning ended. Elapsed time for learning: 450.20845675468445s\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "tstart = time.time()\n",
    "\n",
    "# The experience of the agent is saved as a named tuple containing various variables\n",
    "agentExp = namedtuple(\"exp\", [\"state\", \"action\", \"reward\", \"nextState\", \"done\"])\n",
    "\n",
    "# Parameters\n",
    "nEpisodes = 2000 # Number of learning episodes\n",
    "maxNumTimeSteps = 1000 # The number of time step in each episode\n",
    "gamma = .995 # The discount factor\n",
    "ebsilon = 1 # The starting  value of ebsilon\n",
    "ebsilonEnd   = .1 # The finishing value of ebsilon\n",
    "eDecay = 0.995 # The rate at which ebsilon decays \n",
    "miniBatchSize = 100 # The length of minibatch that is used for training\n",
    "memorySize = 100_000 # The length of the entire memory\n",
    "numUpdateTS = 4 # Frequency of time steps to update the NNs\n",
    "numP_Average = 100 # The number of previous episodes for calculating the average episode reward\n",
    "\n",
    "# Variables for saving the required data for later analysis\n",
    "episodePointHist = [] # For saving each episode's point for later demonstration\n",
    "episodeTimeHist = [] # For saving the time it took for episode to end\n",
    "actionString = \"\" # A string containing consecutive actions taken in an episode (dellimited by comma, i.e. 1,2,4,2,1 etc.)\n",
    "episodeHistDf = pd.DataFrame(columns=[\"episode\", \"actions\", \"seed\", \"points\"])\n",
    "initialCond = None # initial condition (state) of the episode\n",
    "\n",
    "# Making the memory buffer object\n",
    "mem = ReplayMemory(memorySize, __dtype, __device)\n",
    "\n",
    "for episode in range(nEpisodes):\n",
    "    initialSeed = random.randint(1,1_000_000_000) # The random seed that determines the episode's I.C.\n",
    "    state, info = env.reset(seed = initialSeed)\n",
    "    points = 0\n",
    "    actionString = \"\"\n",
    "    initialCond = state\n",
    "    \n",
    "    tempTime = time.time()\n",
    "    for t in range(maxNumTimeSteps):\n",
    "\n",
    "        qValueForActions = qNetwork_model(torch.tensor(state, device = __device, dtype = __dtype))\n",
    "        \n",
    "        # use ebsilon-Greedy algorithm to take the new step\n",
    "        action = getAction(qValueForActions, ebsilon)\n",
    "\n",
    "        # Take a step\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store the experience of the current step in an experience deque.\n",
    "        mem.addNew(\n",
    "            agentExp(\n",
    "                state, # Current state\n",
    "                action, \n",
    "                reward, # Current state's reward\n",
    "                observation, # Next state\n",
    "                True if terminated or truncated else False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # env.render()\n",
    "        # Check to see if we have to update the networks in the current step\n",
    "        update = updateNetworks(t, mem, miniBatchSize, numUpdateTS)\n",
    "\n",
    "        if update:\n",
    "            initial_weights = {name: param.clone() for name, param in qNetwork_model.named_parameters()}\n",
    "            # Update the NNs\n",
    "            experience = mem.sample(miniBatchSize)\n",
    "\n",
    "            # Update teh Q-Network and the target Q-Network\n",
    "            # Bear in mind that we do not update the target Q-network with direct gradient descent.\n",
    "            # so there is no optimizer needed for it\n",
    "            fitQNetworks(experience, gamma, [qNetwork_model, optimizer_main], [targetQNetwork_model, None])\n",
    "\n",
    "        # Save the necessary data\n",
    "        points += reward\n",
    "        state = observation.copy()\n",
    "        actionString += f\"{action},\"\n",
    "\n",
    "        if terminated or truncated:\n",
    "            # Save the episode history in dataframe\n",
    "            if (episode+1)%10 == 0:\n",
    "                # only save every 10 episodes\n",
    "                episodeHistDf = pd.concat([episodeHistDf, pd.DataFrame({\n",
    "                    \"episode\": [episode], \n",
    "                    \"actions\": [actionString], \n",
    "                    \"seed\": [initialSeed], \n",
    "                    \"points\": [points]\n",
    "                })])\n",
    "\n",
    "            break\n",
    "    \n",
    "    # Saving the current episode's points and time\n",
    "    episodePointHist.append(points)\n",
    "    episodeTimeHist.append(time.time()-tempTime)\n",
    "\n",
    "    # Getting the average of {numP_Average} episodes\n",
    "    epPointAvg = np.mean(episodePointHist[-numP_Average:])\n",
    "\n",
    "    # Decay ebsilon\n",
    "    ebsilon = decayEbsilon(ebsilon, eDecay, ebsilonEnd)\n",
    "    \n",
    "    print(f\"\\rElapsedTime: {(time.time() - tstart):.0f}s | Episode: {episode} | The average of the {numP_Average} episodes is: {epPointAvg:.2f}\", end = \" \")\n",
    "\n",
    "    # Stop the learning process if suitable average point is reacheds\n",
    "    if 50 < epPointAvg:\n",
    "        Tend = time.time()\n",
    "        print(f\"\\nThe learning ended. Elapsed time for learning: {Tend-tstart}s\")\n",
    "        break\n",
    "\n",
    "# Reset the index\n",
    "episodeHistDf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = testAgent(\"LunarLander-v2\", qNetwork_model, \"./data/lunarLander-0.gif\")\n",
    "print(f\"Episode point is: {point}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
