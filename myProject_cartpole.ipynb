{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import math\n",
    "import random\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use gymnasium's CartPole-v1 environment. According to the\n",
    "# documentation, we have two actions in the action space, 0 (Go left)\n",
    "# and 1 (Go right) \n",
    "\n",
    "# Also the observation space is a ndarray with shape (4,), It is as\n",
    "# follows:\n",
    "#   0- Cart position [-4.8,4.8]\n",
    "#   1- Cart velocity [-inf,inf]\n",
    "#   2- Pole angle [-.418 rad, .418 rad]\n",
    "#   2- Pole anglular velocity [-inf,inf]\n",
    "\n",
    "# Rewards: Since the goal is to keep the pole upright for as long as \n",
    "#   possible, a reward of +1 for every step taken, including the termination \n",
    "#   step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "# Episode end: The episode ends if any one of the following occurs\n",
    "#   Termination: Pole Angle is greater than ±12°\n",
    "#   Termination: Cart Position is greater than ±2.4 (center of the cart reaches\n",
    "#       the edge of the display)\n",
    "#   Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "# Making the environment\n",
    "env = gym.make(\"CartPole-v1\") # Use render_mode = \"human\" to render each episode\n",
    "state, info = env.reset() # Get a sample state of the environment\n",
    "stateSize = env.observation_space.shape # Number of variables to define current step\n",
    "nActions = env.action_space.n # Number of actions\n",
    "nObs = len(state) # Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Implement's the replay memory algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, size) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the class with a double ended queue which will contain named tuples.\n",
    "        \"\"\"\n",
    "        self.exp = deque([], maxlen=size)\n",
    "        self.len = len(self.exp)\n",
    "    \n",
    "    def addNew(self, exp:namedtuple) -> None:\n",
    "        \"\"\"\n",
    "        Adding a new iteration to the memory. Note thate the most recent values of\n",
    "        the training will be located at the position 0 and if the list reaches maxlen\n",
    "        the oldest data will be dropped.\n",
    "\n",
    "        Args:\n",
    "            exp: namedtuple: The experience should be a named tuple with keys named \n",
    "                likae this: [\"state\", \"action\", \"reward\", \"nextState\", \"done\"]\n",
    "        \"\"\"\n",
    "        self.exp.appendleft(exp)\n",
    "        self.len = len(self.exp)\n",
    "    \n",
    "    def sample(self, miniBatchSize:int) -> tuple:\n",
    "        \"\"\"\n",
    "        Get a random number of experiences from the entire experience memory.\n",
    "        The memory buffer is a double ended queue (AKA deque) of named tuples. To make \n",
    "        this list usable for tensor flow neural networks, this each named tuple inside \n",
    "        the deque has to be unpacked. we use a iterative method to unpack. It may be \n",
    "        inefficient and maybe using pandas can improve this process. one caveat of using\n",
    "        pandas tables instead of deque is expensiveness of appending/deleting rows \n",
    "        (expereiances) from the table.\n",
    "\n",
    "        Args:\n",
    "            miniBatchSize: int: The size of returned the sample\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing state, action, reward, nextState and done\n",
    "        \"\"\"\n",
    "\n",
    "        miniBatch = random.sample(self.exp, miniBatchSize)\n",
    "        state = tf.convert_to_tensor(np.array([e.state for e in miniBatch if e != None]), dtype=tf.float32)\n",
    "        action = tf.convert_to_tensor(np.array([e.action for e in miniBatch if e != None]), dtype=tf.float32)\n",
    "        reward = tf.convert_to_tensor(np.array([e.reward for e in miniBatch if e != None]), dtype=tf.float32)\n",
    "        nextState = tf.convert_to_tensor(np.array([e.nextState for e in miniBatch if e != None]), dtype=tf.float32)\n",
    "        done = tf.convert_to_tensor(np.array([e.done for e in miniBatch if e != None]).astype(np.uint8), dtype=tf.float32)\n",
    "\n",
    "        return tuple((state, action, reward, nextState, done))\n",
    "\n",
    "def decayEbsilon(currE: float, rate:float, minE:float) -> float:\n",
    "    \"\"\"\n",
    "    Decreases ebsilon each time called. It multiplies current ebsilon to decrease rate.\n",
    "    The decreasing is continued untill reaching minE.\n",
    "    \"\"\"\n",
    "    return(max(currE*rate, minE))\n",
    "\n",
    "def computeLoss(experiences:tuple, gamma:float, qNetwork, target_qNetwork): #chkd\n",
    "    \"\"\"\n",
    "    Computes the loss betweeen y targets and Q values. If the reward of current step is R_i,\n",
    "    then y = R_i if the episode is terminated, if not, y = R_i + gamma * Q_hat(i+1) where gamma\n",
    "    is the discount factor and Q_hat is the predicted return of the step i+1 with the target_qNetwork.\n",
    "    \n",
    "    Args:\n",
    "        experiences: Tuple: A tuple containing experiences as tensorflow tensors.\n",
    "        gamma: float: The discount factor.\n",
    "        qNetwork: tensorflow NN: The neural network for predicting the Q.\n",
    "        target_qNetwork: tensorflow NN: The neural network for predicting the target-Q.\n",
    "    \n",
    "    Returns:\n",
    "        loss: float: The Mean squared errors (AKA. MSE) of the Qs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the experience mini-batch\n",
    "    state, action, reward, nextState, done = experiences\n",
    "\n",
    "    # To implement the calculation scheme explained in comments, we multiply Qhat bu (1-done).\n",
    "    # If the episode has termiated done == True so (1-done) = 0.\n",
    "    Qhat = tf.reduce_max(target_qNetwork(nextState), axis=-1)\n",
    "    yTarget = reward + gamma *  Qhat*((1 - done))\n",
    "    \n",
    "    qValues = qNetwork(state)\n",
    "    qValues = tf.gather_nd(qValues, tf.stack([tf.range(qValues.shape[0]), tf.cast(action, tf.int32)], axis=1))\n",
    "    loss = tf.keras.losses.MSE(yTarget, qValues)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def softUpdateNetwork(qNetwork, target_qNetwork, Tau: float):\n",
    "    \"\"\"\n",
    "    Update the target_qNetwork's weights using soft update, This helps the target network to avoid\n",
    "    changing very repidly as qNetwork is updated. This increases the learning srtability greatly.\n",
    "    The formulation is as follows:\n",
    "        W_targetNetwork = W_targetNetwork * (1 - Tau) + Tau * W_qetwork\n",
    "\n",
    "    Args:\n",
    "        qNetwork, target_qNetwork: Tensorflow networks: Self explainatory.\n",
    "        Tau: float: The soft update parameter, Tau << 1\n",
    "    \"\"\"\n",
    "    # Because we use tf.function in this project, we cant use get_waight() or set_weight() functions\n",
    "    # to update the model weights. This is due to the fact that the previouse functions use numpy\n",
    "    # in their structures which is not supported while generating tensorflow graphs. So we for updating \n",
    "    # the weights, we have to iterate throught each and every one and replace them in a loop which is not\n",
    "    # ideal (performance-wise). Below is a way to update the weights in batches but can only be used \n",
    "    # if we are doing the calculations in eagar mode.\n",
    "\n",
    "    # W_target = np.array(target_qNetwork.get_weights(), dtype=object)\n",
    "    # w_qNetwork = np.array(qNetwork.get_weights(), dtype=object)\n",
    "    # W_target = (1-Tau) * W_target + Tau * w_qNetwork\n",
    "    # target_qNetwork.set_weights(W_target)\n",
    "\n",
    "    for target_weights, q_net_weights in zip(target_qNetwork.weights, qNetwork.weights):\n",
    "        target_weights.assign(Tau * q_net_weights + (1.0 - Tau) * target_weights)\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def fitQNetworks(experience, gamma, qNetwork, target_qNetwork, optimizer):\n",
    "    \"\"\"\n",
    "    Updates the weights of the neural networks with a custom training loop. The target network is\n",
    "    updated my a soft update mechanism.\n",
    "\n",
    "    Args: \n",
    "        experience: tuple: The data for training networks. This data has to be passed with \n",
    "            replayMemory.sample() function which returns a tuple of tensorflow tensors in \n",
    "            the following order: state, action, reward, nextState, done)\n",
    "        gamma: flaot: The learning rate.\n",
    "        qNetwork, target_qNetwork: tensorflow NNs\n",
    "        optimizer: tf.keras.optimizers.Adam: The optimizer for updating the networks.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = computeLoss(experience, gamma, qNetwork, target_qNetwork)\n",
    "\n",
    "    grad = tape.gradient(loss, qNetwork.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, qNetwork.trainable_variables))\n",
    "    softUpdateNetwork(qNetwork, target_qNetwork, .001)\n",
    "\n",
    "\n",
    "def getAction(qVal: list, e:float) -> int:\n",
    "    \"\"\"\n",
    "    Gets the action via an ebsilon-greedy algoithm. This entire action state is [0, 1, 2, 3].\n",
    "    With a probability of ebsilon, a random choice will be picked, else the action with\n",
    "    the greatest Q value will be pucked\n",
    "\n",
    "    Args:\n",
    "        qVal: list: The q value of actions\n",
    "        e: float: The ebsilon which represents the probability of a random action\n",
    "    \n",
    "    Returns:\n",
    "        action_: int: 0 for doing nothing, and 1 for left thruster, 2 form main thruster\n",
    "            and 3 for right thruster.\n",
    "    \"\"\"\n",
    "    rnd = random.random()\n",
    "\n",
    "    # The actions possble for cartPole i.e. [One step to left, One step to right]\n",
    "    actions = [0, 1] \n",
    "\n",
    "    if rnd < e:\n",
    "        # Take a random step\n",
    "        action_ = random.randint(0,1)\n",
    "    else:\n",
    "        action_ = actions[np.argmax(qVal)]\n",
    "    \n",
    "    return action_\n",
    "\n",
    "def updateNetworks(timeStep: int, replayMem: ReplayMemory, miniBatchSize: int, C: int) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the neural network (qNetwork and tqrget_qNetwork) weights are to be updated.\n",
    "    The update happens C time steps apart. for performance reasons.\n",
    "\n",
    "    Args:\n",
    "        timeStep: int: The time step of the current episode\n",
    "        replayMem: deque: A double edged queue containing the experperiences as named tuples.\n",
    "            the named tuples should be as follows: [\"state\", \"action\", \"reward\", \"nextState\", \"done\"]\n",
    "\n",
    "    Returns:\n",
    "        A boolean, True for update and False to not update.\n",
    "    \"\"\"\n",
    "\n",
    "    return True if ((timeStep+1) % C == 0 and miniBatchSize < replayMem.len) else False\n",
    "\n",
    "def getEbsilon(e:float, eDecay:float, minE: float) -> float:\n",
    "    \"\"\"\n",
    "    Decay ebsilon for ebsilon-Greedy algorithm. Ebsilon starts with 1 at the beginning of the \n",
    "    learning process which indicates that the agent completely acts on a random basis (AKA \n",
    "    Exploration) but as the learning is continued, the rate at which agent acts randomly decreased\n",
    "    via multiplying the ebsilon by a decay rate which ensures agent acting based on it's learnings\n",
    "    (AKA Exploitation).\n",
    "\n",
    "    Args:\n",
    "        e: float: The current rate of ebsilon\n",
    "        eDecay: float: The decay rate of ebsilon\n",
    "        minE: float: the minimum amount of ebsilon. To ensure the exploration possibility of the \n",
    "            agent, ebsilon shuldn't be less than a certain amount.\n",
    "\n",
    "    Returns: ebsilon's value\n",
    "    \"\"\"\n",
    "\n",
    "    return max(minE, eDecay * e)\n",
    "\n",
    "def renderEpisode(initialState: int, actions:str, envName:str, delay:float = .02) -> None:\n",
    "    \"\"\"\n",
    "    Renders the previousely done episode so the user can see what happened. We use gymanium to \n",
    "    render the environment. All the render is done in the \"human\" mode.\n",
    "\n",
    "    Args:\n",
    "        initialState: int: The initial seed that determine's the initial state of the episode \n",
    "            (The state before we took teh first action)\n",
    "        actions: string: A string of actions delimited by comma (i.e. 1,2,3,1,3, etc.)\n",
    "        env: string: The name of the environment to render the actions, It has to be a gymnasium\n",
    "            compatible environmnet.\n",
    "        delay: int: The delay (In seconds) to put between showing each step to make it more \n",
    "            comprehensive.\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    tempEnv = gym.make(envName, render_mode = \"human\") # Use render_mode = \"human\" to render each episode\n",
    "    state, info = tempEnv.reset(seed=initialState) # Get a sample state of the environment\n",
    "\n",
    "    # Process the string of actions taken\n",
    "    actions = actions.split(\",\") # Split the data\n",
    "    actions = actions[:-1] # Remove the lat Null member of the list\n",
    "    actions = list(map(int, actions)) # Convert the strings to ints\n",
    "\n",
    "    # Take steps\n",
    "    for action in actions:\n",
    "        _, _, terminated, truncated, _ = tempEnv.step(action)\n",
    "    \n",
    "        # Exit loop if the simulation has ended\n",
    "        if terminated or truncated:\n",
    "            _, _ = tempEnv.reset()\n",
    "            break\n",
    "        \n",
    "        # Delay showing the next step\n",
    "        time.sleep(delay)\n",
    "\n",
    "    tempEnv.close()\n",
    "    \n",
    "def testAgent(envName:str, network, saveVideoName:str = \"\") -> int:\n",
    "    \"\"\"\n",
    "    Runs an agent through a predefined gymnasium environment. The actions of the agent are chosen via\n",
    "    a greedy policy by a trained neural network. To see the agent in action, the environment's render\n",
    "    mode has to be \"human\" or  \"rgb-array\"\n",
    "\n",
    "    Args:   \n",
    "        envName: string: The name of the environment.\n",
    "        network: tensorflow NN: The trained neural network that accepts state as an input and outputs\n",
    "            the desired action.\n",
    "        environment: gymnasium env: The environment for testing.\n",
    "        saveVideoName:string: The name of the file to be saved. If equals \"\", No video file will be \n",
    "            saved; Also remember that the file name should include the file extension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def interactionLoop(env_, seed_, V_):    \n",
    "        \"\"\"\n",
    "        The loop that lets agent interact with the environment.\n",
    "        if V_ == True, save the video (requires render_mdeo == rgb_array)\n",
    "        \"\"\"\n",
    "        state, _ = env_.reset(seed = seed_)\n",
    "        points = 0\n",
    "        if V_:\n",
    "            videoWriter = imageio.get_writer(saveVideoName, fps = 30)\n",
    "\n",
    "        maxStepN = 1000\n",
    "        for t in range(maxStepN):\n",
    "            # Take greedy steps\n",
    "            action = np.argmax(network(np.expand_dims(state, axis = 0)))\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env_.step(action)\n",
    "\n",
    "            if V_:\n",
    "                videoWriter.append_data(env_.render())\n",
    "\n",
    "            points += reward\n",
    "\n",
    "            # Exit loop if the simulation has ended\n",
    "            if terminated or truncated:\n",
    "                _, _ = env_.reset()\n",
    "\n",
    "                if V_:\n",
    "                    videoWriter.close()\n",
    "                    \n",
    "                return points\n",
    "    \n",
    "    # Get the random seed to get the initial state of the agent.\n",
    "    seed = random.randint(0, 1_000_000_000)\n",
    "    \n",
    "    # Because gymnasium doesn't let the environment to have two render modes, \n",
    "    # we run the simulation twice, The first renders the environment with \"human\"\n",
    "    # mdoe and the secon run, runs the environment with \"egb_array\" mode that \n",
    "    # lets us save the interaction process to a video file. Both loops are run \n",
    "    # with the same seeds and neural networks so they should have identical outputs.\n",
    "    environment = gym.make(envName, render_mode = \"human\")\n",
    "    point = interactionLoop(environment, seed, False)\n",
    "            \n",
    "    environment = gym.make(envName, render_mode = \"rgb_array\")\n",
    "    point = interactionLoop(environment, seed, True)\n",
    "\n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the neural networks\n",
    "# Specifying the optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(.001)\n",
    "\n",
    "# Because of the binary structure of the actions possible (Step to right or left) we have\n",
    "# added a taken a sigmoid activation for the output layer.\n",
    "\n",
    "# The q-Network\n",
    "inputs = tf.keras.layers.Input(shape=stateSize)\n",
    "l1 = tf.keras.layers.Dense(units=64, activation=\"relu\", name = \"L1\")(inputs)\n",
    "l2 = tf.keras.layers.Dense(units=64, activation=\"relu\", name = \"L2\")(l1)\n",
    "output = tf.keras.layers.Dense(units=nActions, activation=\"linear\", name = \"out\")(l2)\n",
    "qNetwork = tf.keras.Model(inputs = inputs, outputs = output, name = \"q-network\")\n",
    "\n",
    "# The Target q-Network\n",
    "target_inputs = tf.keras.layers.Input(shape=stateSize)\n",
    "target_l1 = tf.keras.layers.Dense(units=64, activation=\"relu\", name = \"L1\")(target_inputs)\n",
    "target_l2 = tf.keras.layers.Dense(units=64, activation=\"relu\", name = \"L2\")(target_l1)\n",
    "target_output = tf.keras.layers.Dense(units=nActions, activation=\"linear\", name = \"out\")(target_l2)\n",
    "target_qNetwork = tf.keras.Model(inputs = target_inputs, outputs = target_output, name = \"target-q-network\")\n",
    "\n",
    "# Both qnetwork and target-qnetworks's weights are initialized but at the ebginning\n",
    "# the two networks should have the same weights.\n",
    "target_qNetwork.set_weights(qNetwork.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElapsedTime: 262s | Episode: 1780 | The average of the 100 episodes is: 92.31  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 42\u001b[0m\n\u001b[1;32m     36\u001b[0m tempTime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(maxNumTimeSteps):\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m     \u001b[39m# We use teh __call__ function instead of predict() method because the predict() \u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39m# function is better suited (faster) for batches of data but we here only have \u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[39m# a single state to predict.\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     qValueForActions \u001b[39m=\u001b[39m qNetwork(np\u001b[39m.\u001b[39;49mexpand_dims(state, axis \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m))\n\u001b[1;32m     44\u001b[0m     \u001b[39m# use ebsilon-Greedy algorithm to take the new step\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     action \u001b[39m=\u001b[39m getAction(qValueForActions, ebsilon)\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/engine/training.py:558\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    556\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 558\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/engine/functional.py:512\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    495\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \n\u001b[1;32m    497\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/engine/functional.py:669\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 669\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlayer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    673\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[1;32m    674\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/engine/base_layer.py:1123\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     name_scope \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_unnested_name_scope()\n\u001b[1;32m   1121\u001b[0m     call_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autographed_call()\n\u001b[0;32m-> 1123\u001b[0m call_fn \u001b[39m=\u001b[39m traceback_utils\u001b[39m.\u001b[39;49minject_argument_info_in_traceback(\n\u001b[1;32m   1124\u001b[0m     call_fn,\n\u001b[1;32m   1125\u001b[0m     object_name\u001b[39m=\u001b[39;49m(\n\u001b[1;32m   1126\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlayer \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m (type \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m   1127\u001b[0m     ),\n\u001b[1;32m   1128\u001b[0m )\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mExitStack() \u001b[39mas\u001b[39;00m namescope_stack:\n\u001b[1;32m   1130\u001b[0m     \u001b[39mif\u001b[39;00m _is_name_scope_on_model_declaration_enabled:\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:160\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback\u001b[0;34m(fn, object_name)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mdel\u001b[39;00m signature\n\u001b[1;32m    158\u001b[0m         \u001b[39mdel\u001b[39;00m bound_signature\n\u001b[0;32m--> 160\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdecorator\u001b[39m.\u001b[39;49mmake_decorator(fn, error_handler)\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/tensorflow/python/util/tf_decorator.py:168\u001b[0m, in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   bound_instance \u001b[39m=\u001b[39m _get_bound_instance(target)\n\u001b[1;32m    169\u001b[0m   \u001b[39m# Present the decorated func as a method as well\u001b[39;00m\n\u001b[1;32m    170\u001b[0m   \u001b[39mif\u001b[39;00m bound_instance \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mself\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m signature\u001b[39m.\u001b[39mparameters:\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/tensorflow/python/util/tf_decorator.py:181\u001b[0m, in \u001b[0;36m_get_bound_instance\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_bound_instance\u001b[39m(target):\n\u001b[1;32m    180\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns the instance any of the targets is attached to.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m   decorators, target \u001b[39m=\u001b[39m unwrap(target)\n\u001b[1;32m    182\u001b[0m   \u001b[39mfor\u001b[39;00m decorator \u001b[39min\u001b[39;00m decorators:\n\u001b[1;32m    183\u001b[0m     \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39mismethod(decorator\u001b[39m.\u001b[39mdecorated_target):\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/tensorflow/python/util/tf_decorator.py:291\u001b[0m, in \u001b[0;36munwrap\u001b[0;34m(maybe_tf_decorator)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(cur, TFDecorator):\n\u001b[1;32m    290\u001b[0m   decorators\u001b[39m.\u001b[39mappend(cur)\n\u001b[0;32m--> 291\u001b[0m \u001b[39melif\u001b[39;00m _has_tf_decorator_attr(cur):\n\u001b[1;32m    292\u001b[0m   decorators\u001b[39m.\u001b[39mappend(\u001b[39mgetattr\u001b[39m(cur, \u001b[39m'\u001b[39m\u001b[39m_tf_decorator\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    293\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/MainCode/My-Data-Science-Learning-Journy/env/lib/python3.10/site-packages/tensorflow/python/util/tf_decorator.py:196\u001b[0m, in \u001b[0;36m_has_tf_decorator_attr\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_has_tf_decorator_attr\u001b[39m(obj):\n\u001b[1;32m    188\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Checks if object has _tf_decorator attribute.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[39m  This check would work for mocked object as well since it would\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m    obj: Python object.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m   \u001b[39mreturn\u001b[39;00m (\u001b[39mhasattr\u001b[39;49m(obj, \u001b[39m'\u001b[39;49m\u001b[39m_tf_decorator\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    197\u001b[0m           \u001b[39misinstance\u001b[39m(\u001b[39mgetattr\u001b[39m(obj, \u001b[39m'\u001b[39m\u001b[39m_tf_decorator\u001b[39m\u001b[39m'\u001b[39m), TFDecorator))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "tstart = time.time()\n",
    "\n",
    "# The experience of the agent is saved as a named tuple containing variouse variables\n",
    "agentExp = namedtuple(\"exp\", [\"state\", \"action\", \"reward\", \"nextState\", \"done\"])\n",
    "\n",
    "# Parameters\n",
    "nEpisodes = 5000 # Number of learning episodes\n",
    "maxNumTimeSteps = 2000 # The number of time step in each episode\n",
    "gamma = .995 # The discount factor\n",
    "ebsilon = 1 # The starting  value of ebsilon\n",
    "ebsilonEnd   = .1 # The finishing value of ebsilon\n",
    "eDecay = 0.995 # The rate at which ebsilon decays \n",
    "miniBatchSize = 400 # The length of minibatch that is used for training\n",
    "memorySize = 1_000_000 # The length of the entire memory\n",
    "numUpdateTS = 4 # Frequency of timesteps to update the NNs\n",
    "numP_Average = 100 # The number of previouse episodes for calculating the average episode reward\n",
    "\n",
    "# Variables for saving the required data for later analysis\n",
    "episodePointHist = [] # For saving each episode's point for later demonstration\n",
    "episodeTimeHist = [] # For saving the time it took for episode to end\n",
    "actionString = \"\" # A string containing consecutive actions taken in an episode (dellimited by comma, i.e. 1,2,4,2,1 etc.)\n",
    "episodeHistDf = pd.DataFrame(columns=[\"episode\", \"actions\", \"seed\", \"points\"])\n",
    "initialCond = None # initial condition (state) of the episode\n",
    "\n",
    "# Making the memory buffer object\n",
    "mem = ReplayMemory(size=memorySize)\n",
    "\n",
    "for episode in range(nEpisodes):\n",
    "    initialSeed = random.randint(1,1_000_000_000) # The random seed that determines the episode's I.C.\n",
    "    state, info = env.reset(seed = initialSeed)\n",
    "    points = 0\n",
    "    actionString = \"\"\n",
    "    initialCond = state\n",
    "    \n",
    "    tempTime = time.time()\n",
    "    for t in range(maxNumTimeSteps):\n",
    "\n",
    "        # We use teh __call__ function instead of predict() method because the predict() \n",
    "        # function is better suited (faster) for batches of data but we here only have \n",
    "        # a single state to predict.\n",
    "        qValueForActions = qNetwork(np.expand_dims(state, axis = 0))\n",
    "\n",
    "        # use ebsilon-Greedy algorithm to take the new step\n",
    "        action = getAction(qValueForActions, ebsilon)\n",
    "\n",
    "        # Take a step\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store the experience of the current step in an experience deque.\n",
    "        mem.addNew(\n",
    "            agentExp(\n",
    "                state, # Current state\n",
    "                action, \n",
    "                reward, # Current state's reward\n",
    "                observation, # Next state\n",
    "                True if terminated or truncated else False\n",
    "            )\n",
    "        )\n",
    "        env.render()\n",
    "        # Check to see if we have to update the networks in the current step\n",
    "        update = updateNetworks(t, mem, miniBatchSize, numUpdateTS)\n",
    "\n",
    "        if update:\n",
    "            # Update the NNs\n",
    "            experience = mem.sample(miniBatchSize)\n",
    "            fitQNetworks(experience, gamma, qNetwork, target_qNetwork, optimizer)\n",
    "\n",
    "        # Save the necessary data\n",
    "        points += reward\n",
    "        state = observation.copy()\n",
    "        actionString += f\"{action},\"\n",
    "\n",
    "        if terminated or truncated:\n",
    "            # Save the episode histry in dataframe\n",
    "            if (episode+1)%10 == 0:\n",
    "                # only save every 10 episodes\n",
    "                episodeHistDf = pd.concat([episodeHistDf, pd.DataFrame({\n",
    "                    \"episode\": [episode], \n",
    "                    \"actions\": [actionString], \n",
    "                    \"seed\": [initialSeed], \n",
    "                    \"points\": [points]\n",
    "                })])\n",
    "\n",
    "            break\n",
    "    \n",
    "    # Saving the current episode's points and time\n",
    "    episodePointHist.append(points)\n",
    "    episodeTimeHist.append(time.time()-tempTime)\n",
    "\n",
    "    # Getting the average of {numP_Average} episodes\n",
    "    epPointAvg = np.mean(episodePointHist[-numP_Average:])\n",
    "\n",
    "    # Decay ebsilon\n",
    "    ebsilon = decayEbsilon(ebsilon, eDecay, ebsilonEnd)\n",
    "    \n",
    "    print(f\"\\rElapsedTime: {(time.time() - tstart):.0f}s | Episode: {episode} | The average of the {numP_Average} episodes is: {epPointAvg:.2f}\", end = \" \")\n",
    "\n",
    "    if 475 < epPointAvg:\n",
    "        Tend = time.time()\n",
    "        print(f\"\\nThe learning ended. Elapsed time for learning: {Tend-tstart}s\")\n",
    "        break\n",
    "\n",
    "# Reset the index\n",
    "episodeHistDf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[swscaler @ 0x6674800] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAgent(\"CartPole-v1\", qNetwork, \"data/cartPole.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>actions</th>\n",
       "      <th>seed</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0,1,1,1,1,1,1,1,1,1,0,0,</td>\n",
       "      <td>436613231</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>1,1,1,1,1,0,1,0,0,0,0,</td>\n",
       "      <td>240820538</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>1,0,0,0,1,1,0,0,1,0,1,1,1,0,1,1,0,1,1,0,1,0,1,...</td>\n",
       "      <td>326165113</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,</td>\n",
       "      <td>836462868</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>1,0,1,0,0,0,0,0,0,0,1,0,0,1,</td>\n",
       "      <td>199704332</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1739</td>\n",
       "      <td>0,0,0,0,1,1,1,1,0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,...</td>\n",
       "      <td>717077061</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1749</td>\n",
       "      <td>1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,1,1,1,0,1,0,...</td>\n",
       "      <td>644292864</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1759</td>\n",
       "      <td>1,1,1,1,0,0,1,0,1,0,0,1,0,0,0,1,0,0,0,1,1,1,0,...</td>\n",
       "      <td>245945233</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1769</td>\n",
       "      <td>1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,...</td>\n",
       "      <td>801375716</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1779</td>\n",
       "      <td>1,1,0,1,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,1,0,...</td>\n",
       "      <td>678125001</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode                                            actions       seed  \\\n",
       "0         9                           0,1,1,1,1,1,1,1,1,1,0,0,  436613231   \n",
       "1        19                             1,1,1,1,1,0,1,0,0,0,0,  240820538   \n",
       "2        29  1,0,0,0,1,1,0,0,1,0,1,1,1,0,1,1,0,1,1,0,1,0,1,...  326165113   \n",
       "3        39                   1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,  836462868   \n",
       "4        49                       1,0,1,0,0,0,0,0,0,0,1,0,0,1,  199704332   \n",
       "..      ...                                                ...        ...   \n",
       "173    1739  0,0,0,0,1,1,1,1,0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,...  717077061   \n",
       "174    1749  1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,1,1,1,0,1,0,...  644292864   \n",
       "175    1759  1,1,1,1,0,0,1,0,1,0,0,1,0,0,0,1,0,0,0,1,1,1,0,...  245945233   \n",
       "176    1769  1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,...  801375716   \n",
       "177    1779  1,1,0,1,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,1,0,...  678125001   \n",
       "\n",
       "     points  \n",
       "0      12.0  \n",
       "1      11.0  \n",
       "2      33.0  \n",
       "3      16.0  \n",
       "4      14.0  \n",
       "..      ...  \n",
       "173    53.0  \n",
       "174   107.0  \n",
       "175   107.0  \n",
       "176    99.0  \n",
       "177   101.0  \n",
       "\n",
       "[178 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodeHistDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderEpisode(\n",
    "    episodeHistDf.iloc[169].seed,\n",
    "    episodeHistDf.iloc[169].actions,\n",
    "    \"CartPole-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
